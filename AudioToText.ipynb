{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f65ac3",
   "metadata": {},
   "source": [
    "# Audio Recording, Diarization, and Transcription System\n",
    "\n",
    "This project provides functionality to record audio, perform speaker diarization, and transcribe the speech content from recorded audio using libraries such as **PyAudio**, **Wave**, **Vosk**, **Whisper**, and **Pydub**. Below is detailed documentation on how this system works, the libraries it uses, and the models required.\n",
    "\n",
    "---\n",
    "\n",
    "## **Features**\n",
    "1. **Audio Recording**  \n",
    "   Records live audio input from the microphone for a specified duration.\n",
    "   \n",
    "2. **Speaker Diarization**  \n",
    "   Identifies and segments speech from different speakers in the recorded audio.\n",
    "\n",
    "3. **Speech Transcription**  \n",
    "   Converts the segmented audio from speaker diarization into text.\n",
    "   \n",
    "---\n",
    "\n",
    "## **Libraries Used**\n",
    "\n",
    "### **1. PyAudio**\n",
    "   - **Purpose**: Handles live audio recording from the microphone.\n",
    "   - **Installation**:  \n",
    "     ```bash\n",
    "     pip install pyaudio\n",
    "     ```\n",
    "   - **Documentation**: [PyAudio Documentation](https://people.csail.mit.edu/hubert/pyaudio/)\n",
    "\n",
    "### **2. Wave**\n",
    "   - **Purpose**: Reads and writes WAV audio files.\n",
    "   - **Installation**: Built into Python; no separate installation is required.\n",
    "   - **Documentation**: [Wave Documentation](https://docs.python.org/3/library/wave.html)\n",
    "\n",
    "### **3. Pydub**\n",
    "   - **Purpose**: Manages audio processing tasks such as audio segmentation.\n",
    "   - **Installation**:  \n",
    "     ```bash\n",
    "     pip install pydub\n",
    "     ```\n",
    "\n",
    "   - **Documentation**: [Pydub Documentation](https://github.com/jiaaro/pydub)\n",
    "\n",
    "### **4. Vosk**\n",
    "   - **Purpose**: Provides speaker diarization using a pre-trained Vosk model.\n",
    "   - **Installation**:  \n",
    "     ```bash\n",
    "     pip install vosk\n",
    "     ```\n",
    "   - **Documentation**: [Vosk API Documentation](https://alphacephei.com/vosk/)\n",
    "\n",
    "### **5. Whisper**\n",
    "   - **Purpose**: Performs automatic speech recognition (ASR) for transcription.\n",
    "   - **Installation**:  \n",
    "     ```bash\n",
    "     pip install openai-whisper\n",
    "     ```\n",
    "   - **Dependencies**: Requires PyTorch. Install PyTorch as per your system configuration:  \n",
    "     ```bash\n",
    "     pip install torch torchvision torchaudio\n",
    "     ```\n",
    "   - **Documentation**: [Whisper GitHub Repository](https://github.com/openai/whisper)\n",
    "\n",
    "---\n",
    "\n",
    "## **Models Used**\n",
    "\n",
    "### **1. Whisper ASR Model**\n",
    "   - **Source**: OpenAI's Whisper\n",
    "   - **Purpose**: Converts audio to text with high accuracy.\n",
    "   - **Model Variant Used**: `base`\n",
    "   - **Documentation**: [Whisper Models](https://github.com/openai/whisper#available-models)\n",
    "\n",
    "   ```python\n",
    "   model = whisper.load_model(\"base\")\n",
    "   ```\n",
    "\n",
    "### **2. Vosk Speaker Diarization Model**\n",
    "   - **Source**: Vosk\n",
    "   - **Purpose**: Identifies and segments audio by speakers.\n",
    "   - **Model Used**: `vosk-model-small-en-us-0.15`\n",
    "   - **Download**: [Vosk Models](https://alphacephei.com/vosk/models)\n",
    "   - **Model Path Example**: `\"vosk-model-small-en-us-0.15\"`\n",
    "\n",
    "---\n",
    "\n",
    "## **Code Walkthrough**\n",
    "\n",
    "### **1. Recording Audio**\n",
    "The `record_audio` function captures audio from the system microphone and saves it as a WAV file.\n",
    "\n",
    "#### **Parameters**\n",
    "- `output_filename` (str): Path to save the recorded audio file.\n",
    "- `duration` (int): Duration of the recording in seconds.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `p.open`: Opens the audio stream for recording.\n",
    "- `stream.read`: Reads audio chunks and saves them to a list.\n",
    "- `wave.open`: Saves the recorded audio in WAV format.\n",
    "\n",
    "#### Example Call\n",
    "```python\n",
    "audio_filename = \"live_audio.wav\"\n",
    "record_audio(audio_filename, duration=10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Diarizing Audio**\n",
    "The `diarize_audio` function segments audio into speaker-specific parts using Vosk.\n",
    "\n",
    "#### **Parameters**\n",
    "- `audio_file_path` (str): Path to the audio file to process.\n",
    "- `model_path` (str): Path to the Vosk model directory.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `KaldiRecognizer`: Processes the WAV file and identifies text and speaker segments.\n",
    "- Outputs a list of tuples in the format `(start_time, end_time, speaker)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Transcribing Audio**\n",
    "The `transcribe_audio` function uses the Whisper model to transcribe audio into text.\n",
    "\n",
    "#### **Parameters**\n",
    "- `segment_path` (str): Path to the audio segment file for transcription.\n",
    "\n",
    "#### **Key Functions**\n",
    "- `model.transcribe`: Converts the audio segment into text.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Processing Audio**\n",
    "The `process_audio` function combines diarization, transcription, and playback.\n",
    "\n",
    "#### **Steps**\n",
    "1. Call `diarize_audio` to get speaker segments.\n",
    "2. Extract individual audio segments using Pydub.\n",
    "3. Transcribe each segment using Whisper.\n",
    "4. Print the results grouped by speaker.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example Workflow**\n",
    "\n",
    "```python\n",
    "audio_filename = \"live_audio.wav\"\n",
    "record_audio(audio_filename, duration=50)  \n",
    "process_audio(audio_filename, model_path=\"vosk-model-small-en-us-0.15\") \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Outputs**\n",
    "1. **Segmented Audio Playback**  \n",
    "   Plays each audio segment corresponding to a specific speaker.\n",
    "   \n",
    "2. **Transcription**  \n",
    "   Prints the transcription of audio for each speaker. Example:\n",
    "   ```\n",
    "   Person1: Hello, how are you?\n",
    "   Person2: I'm good, thank you.\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a6d57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Recording finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jawaria/Documents/NLP-Project/env/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1823.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing segment for SPEAKER_01 from 0.03s to 0.52s...\n",
      "Playing segment for SPEAKER_01 from 1.04s to 3.56s...\n",
      "Playing segment for SPEAKER_00 from 3.86s to 6.83s...\n",
      "Playing segment for SPEAKER_01 from 6.83s to 11.46s...\n",
      "Playing segment for SPEAKER_00 from 11.74s to 17.11s...\n",
      "Playing segment for SPEAKER_01 from 17.61s to 22.58s...\n",
      "Playing segment for SPEAKER_00 from 22.93s to 27.66s...\n",
      "Playing segment for SPEAKER_01 from 27.94s to 32.48s...\n",
      "Playing segment for SPEAKER_00 from 32.90s to 36.11s...\n",
      "Person2:   \n",
      "Person2: Hey, have you started preparing for the NLP final?  \n",
      "Person1: Not yet, I have been caught up with the project. What about you?  \n",
      "Person2: Same here, the group project is taking all my time and I haven't even started doing the theory.  \n",
      "Person1: Exactly, the project is related to NLP but the final is more about concepts like embedding and Transformers.  \n",
      "Person2: Yeah, and I barely remember anything about the attention mechanism. I think I need to revise the basics.  \n",
      "Person1: Me too. How about we split the topics? You focus on embedding and I'll go over transformers.  \n",
      "Person2: That sounds good. Let's also look at the passapres to see what topics usually come up.  \n",
      "Person1: Good idea, we can meet tomorrow to go over everything. We've got this.  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "\n",
    "\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "pyannote_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"your_huggingface_token\"\n",
    ")\n",
    "\n",
    "def record_audio(output_filename, duration=10):\n",
    "    \"\"\"\n",
    "    Records audio from the microphone and saves it as a .wav file.\n",
    "    \"\"\"\n",
    "    chunk = 1024\n",
    "    sample_format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "    fs = 16000\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    print(\"Recording...\")\n",
    "\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=fs,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "\n",
    "    frames = []\n",
    "    for _ in range(0, int(fs / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    print(\"Recording finished\")\n",
    "\n",
    "    wf = wave.open(output_filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "def diarize_with_vosk(audio_file_path, model_path=\"path_to_your_vosk_model\"):\n",
    "    \"\"\"\n",
    "    Diarizes audio using the Vosk model.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise ValueError(\"Model path does not exist. Please provide a valid path to the Vosk model.\")\n",
    "\n",
    "    model = Model(model_path)\n",
    "    wf = wave.open(audio_file_path, \"rb\")\n",
    "\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getframerate() != 16000:\n",
    "        raise ValueError(\"Audio file must be WAV format mono PCM with 16kHz sample rate.\")\n",
    "\n",
    "    recognizer = KaldiRecognizer(model, wf.getframerate())\n",
    "    recognizer.SetWords(True)\n",
    "\n",
    "    segments = []\n",
    "    start_time = 0.0\n",
    "\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "\n",
    "        if recognizer.AcceptWaveform(data):\n",
    "            result = json.loads(recognizer.Result())\n",
    "            if \"text\" in result and result[\"text\"]:\n",
    "                end_time = wf.tell() / wf.getframerate()\n",
    "                segments.append((start_time, end_time, \"Speaker_1\"))\n",
    "                start_time = end_time\n",
    "\n",
    "    wf.close()\n",
    "    return segments\n",
    "\n",
    "\n",
    "def diarize_with_pyannote(audio_file_path):\n",
    "    \"\"\"\n",
    "    Diarizes audio using Pyannote.\n",
    "    \"\"\"\n",
    "    diarization = pyannote_pipeline(audio_file_path)\n",
    "    return diarization\n",
    "\n",
    "def transcribe_audio(segment_path):\n",
    "    \"\"\"\n",
    "    Transcribes audio using Whisper.\n",
    "    \"\"\"\n",
    "    result = whisper_model.transcribe(segment_path)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def process_audio_with_segments(audio_file_path, segments, speaker_mapping=None):\n",
    "    \"\"\"\n",
    "    Processes audio segments for playback and transcription.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(audio_file_path)\n",
    "    dialogue = \"\"\n",
    "\n",
    "    for i, (start_time, end_time, speaker) in enumerate(segments):\n",
    "        start_ms = int(start_time * 1000)\n",
    "        end_ms = int(end_time * 1000)\n",
    "        segment_audio = audio[start_ms:end_ms]\n",
    "        segment_path = f\"temp_segment_{speaker}_{i}.wav\"\n",
    "        segment_audio.export(segment_path, format=\"wav\")\n",
    "\n",
    "        print(f\"Playing segment for {speaker} from {start_time:.2f}s to {end_time:.2f}s...\")\n",
    "        play(segment_audio)\n",
    "\n",
    "        transcribed_text = transcribe_audio(segment_path)\n",
    "        speaker_name = speaker_mapping.get(speaker, speaker) if speaker_mapping else speaker\n",
    "        dialogue += f\"{speaker_name}: {transcribed_text.strip()}  \\n\"\n",
    "\n",
    "        os.remove(segment_path)\n",
    "\n",
    "    dialogue += \"\\n\"\n",
    "    return dialogue\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_filename = \"live_audio2.wav\"\n",
    "    record_audio(audio_filename, duration=50)\n",
    "\n",
    "    use_vosk = False\n",
    "\n",
    "    if use_vosk:\n",
    "        vosk_model_path = \"vosk-model-small-en-us-0.15\"\n",
    "        segments = diarize_with_vosk(audio_filename, vosk_model_path)\n",
    "        speaker_map = None  \n",
    "    else:\n",
    "        diarization = diarize_with_pyannote(audio_filename)\n",
    "        segments = [(turn.start, turn.end, speaker) for turn, _, speaker in diarization.itertracks(yield_label=True)]\n",
    "        speaker_map = {\"SPEAKER_00\": \"Person1\", \"SPEAKER_01\": \"Person2\"}\n",
    "\n",
    "    dialogue = process_audio_with_segments(audio_filename, segments, speaker_map)\n",
    "    print(dialogue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308bc09",
   "metadata": {},
   "source": [
    "## Documentation: Loading and Testing a Fine-Tuned T5 Model\n",
    "\n",
    "Here we load our fine-tuned T5 model and its tokenizer. The `AutoTokenizer` and `AutoModelForSeq2SeqLM` classes are utilized to load the model and tokenizer from our specified local directory. The model is moved to the GPU if available, and the conversation converted from audio to text is passed to it for summarization. The input prompt includes the conversation text along with a directive to summarize it. The model's output is compared to a human-generated baseline summary to evaluate its performance.\n",
    "\n",
    "Key components include creating tokenized inputs for the model, generating the summary with a specified token limit, and decoding the output into human-readable text. The script outputs the input prompt, a human baseline summary, and the model-generated summary for comparison. This setup is ideal for testing fine-tuned T5 models in tasks like dialogue summarization or similar sequence-to-sequence applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72fb0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "----------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "Person2:   \n",
      "Person2: Hey, have you started preparing for the NLP final?  \n",
      "Person1: Not yet, I have been caught up with the project. What about you?  \n",
      "Person2: Same here, the group project is taking all my time and I haven't even started doing the theory.  \n",
      "Person1: Exactly, the project is related to NLP but the final is more about concepts like embedding and Transformers.  \n",
      "Person2: Yeah, and I barely remember anything about the attention mechanism. I think I need to revise the basics.  \n",
      "Person1: Me too. How about we split the topics? You focus on embedding and I'll go over transformers.  \n",
      "Person2: That sounds good. Let's also look at the passapres to see what topics usually come up.  \n",
      "Person1: Good idea, we can meet tomorrow to go over everything. We've got this.  \n",
      "\n",
      "\n",
      "\n",
      "Summary:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - FINE-TUNED:\n",
      "Person1 and Person2 haven't started preparing for the NLP final. They will split the topics and discuss the passapres.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_save_path = \"./fine_tunede7_t5_model\"\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(model_save_path)\n",
    "model_loaded = AutoModelForSeq2SeqLM.from_pretrained(model_save_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_loaded = model_loaded.to(device)  \n",
    "\n",
    "inputs = tokenizer_loaded(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "output = tokenizer_loaded.decode(\n",
    "    model_loaded.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "\n",
    "dash_line = '-' * 100\n",
    "print(dash_line)\n",
    "print(f\"INPUT PROMPT:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"MODEL GENERATION - FINE-TUNED:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c72819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Audio (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
